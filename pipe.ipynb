{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import json\n",
    "import hashlib\n",
    "import logging\n",
    "import tempfile\n",
    "import datetime\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "#import s3fs\n",
    "#from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type\n",
    "from dotenv import load_dotenv\n",
    "#from google.oauth2 import service_account\n",
    "#from googleapiclient.discovery import build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "SECRET_ACCESS_KEY = os.getenv(\"SECRET_ACCESS_KEY\")\n",
    "ACCESS_KEY_ID = os.getenv(\"ACCESS_KEY_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTINGS\n",
    "AWS_REGION = \"eu-north-1\"   # Stockholm Region\n",
    "LOCAL_TMP = \"./tmp\"\n",
    "\n",
    "# S3 PATHS\n",
    "CUSTOMERS_S3_PATH = \"s3://core-telecoms-data-lake/customers/\"\n",
    "CALL_LOGS_PREFIX = \"s3://core-telecoms-data-lake/call logs/\"  # contains daily CSV\n",
    "SOCIAL_MEDIA_PREFIX = \"s3://core-telecoms-data-lake/social_medias/\"   # contains daily JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOOGLE SHEET SETTINGS\n",
    "GOOGLE_SHEET_ID = \"17IXo7TjDSSHaFobGG9hcqgbsNKTaqgyctWGnwDeNkIQ\"\n",
    "GOOGLE_SHEET_RANGE = \"Agents!A1:D101\"\n",
    "\n",
    "# DAILY POSTGRES TABLES\n",
    "POSTGRES_TABLES = [\n",
    "    \"Web_form_request_2025_11_20\",\n",
    "    \"Web_form_request_2025_11_21\",\n",
    "    \"Web_form_request_2025_11_22\",\n",
    "    \"Web_form_request_2025_11_23\",\n",
    "]\n",
    "POSTGRES_SSM_PARAMETER = \"/coretelecomms/database/\"  # JSON stored here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOGGING SETUP\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPERS\n",
    "def ensure_tmp_folder():\n",
    "    if not os.path.exists(LOCAL_TMP):\n",
    "        os.makedirs(LOCAL_TMP)\n",
    "        logging.info(\"Created local tmp folder.\")\n",
    "    else:\n",
    "        logging.info(\"Local tmp folder already exists.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS HELPERS\n",
    "def get_ssm_secret(name):\n",
    "    \"\"\"Retrieves DB credentials stored as JSON in SSM Parameter Store\"\"\"\n",
    "    ssm = boto3.client(\"ssm\", region_name=AWS_REGION,\\\n",
    "                    aws_access_key_id=ACCESS_KEY_ID,\\\n",
    "                    aws_secret_access_key=SECRET_ACCESS_KEY)\n",
    "    param = ssm.get_parameter(Name=name, WithDecryption=True)\n",
    "    return json.loads(param[\"Parameter\"][\"Value\"])\n",
    "\n",
    "\n",
    "def download_s3_file(s3_path, local_path):\n",
    "    \"\"\"Downloads a single object from S3.\"\"\"\n",
    "    s3 = boto3.client(\"s3\", region_name=AWS_REGION,\\\n",
    "                    aws_access_key_id=ACCESS_KEY_ID,\\\n",
    "                    aws_secret_access_key=SECRET_ACCESS_KEY)\n",
    "\n",
    "    bucket, key = s3_path.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "    s3.download_file(bucket, key, local_path)\n",
    "\n",
    "    logging.info(f\"Downloaded S3 file → {local_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_s3_prefix(prefix, local_dir, file_type=\"csv\"):\n",
    "    \"\"\"Downloads all objects under a folder/prefix.\"\"\"\n",
    "    s3 = boto3.client(\"s3\", region_name=AWS_REGION,\\\n",
    "                    aws_access_key_id=ACCESS_KEY_ID,\\\n",
    "                    aws_secret_access_key=SECRET_ACCESS_KEY)\n",
    "    bucket, key_prefix = prefix.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "\n",
    "    response = s3.list_objects_v2(Bucket=bucket, Prefix=key_prefix)\n",
    "\n",
    "    if \"Contents\" not in response:\n",
    "        logging.warning(f\"No files found under prefix {prefix}\")\n",
    "        return []\n",
    "\n",
    "    downloaded_files = []\n",
    "\n",
    "    for obj in response[\"Contents\"]:\n",
    "        key = obj[\"Key\"]\n",
    "        if not key.endswith(file_type):\n",
    "            continue\n",
    "\n",
    "        filename = key.split(\"/\")[-1]\n",
    "        local_path = os.path.join(local_dir, filename)\n",
    "        s3.download_file(bucket, key, local_path)\n",
    "        downloaded_files.append(local_path)\n",
    "\n",
    "        logging.info(f\"Downloaded {file_type.upper()} from → {local_path}\")\n",
    "\n",
    "    return downloaded_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POSTGRES EXTRACTION\n",
    "def extract_postgres_table(table_name, credentials):\n",
    "    \"\"\"Extracts a single Postgres table into a pandas DataFrame.\"\"\"\n",
    "    conn = psycopg2.connect(\n",
    "        host=credentials[\"host\"],\n",
    "        user=credentials[\"user\"],\n",
    "        password=credentials[\"password\"],\n",
    "        database=credentials[\"database\"],\n",
    "        port=credentials[\"port\"]\n",
    "    )\n",
    "\n",
    "    query = f\"SELECT * FROM customer_complaints.{table_name} LIMIT 5;\"\n",
    "    df = pd.read_sql(query, conn)\n",
    "    conn.close()\n",
    "\n",
    "    logging.info(f\"Extracted Postgres table → {table_name}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 22:31:25,096 | INFO | Local tmp folder already exists.\n"
     ]
    }
   ],
   "source": [
    "ensure_tmp_folder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 22:38:23,213 | INFO | Downloaded S3 file → ./tmp\\customers.csv\n"
     ]
    }
   ],
   "source": [
    "# Customers (Static CSV)\n",
    "customers_path = os.path.join(LOCAL_TMP, \"customers.csv\")\n",
    "download_s3_file(CUSTOMERS_S3_PATH, customers_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 11:40:22,978 | INFO | Downloaded CSV from → ./tmp\\call_logs_day_2025-11-20.csv\n",
      "2025-12-04 11:41:08,865 | INFO | Downloaded CSV from → ./tmp\\call_logs_day_2025-11-21.csv\n",
      "2025-12-04 11:41:58,258 | INFO | Downloaded CSV from → ./tmp\\call_logs_day_2025-11-22.csv\n",
      "2025-12-04 11:42:46,383 | INFO | Downloaded CSV from → ./tmp\\call_logs_day_2025-11-23.csv\n"
     ]
    }
   ],
   "source": [
    "# Call Center Logs (Daily CSV)\n",
    "call_logs = download_s3_prefix(CALL_LOGS_PREFIX, LOCAL_TMP, file_type=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 11:45:39,787 | INFO | Downloaded JSON from → ./tmp\\media_complaint_day_2025-11-20.json\n",
      "2025-12-04 11:46:04,397 | INFO | Downloaded JSON from → ./tmp\\media_complaint_day_2025-11-21.json\n",
      "2025-12-04 11:46:22,631 | INFO | Downloaded JSON from → ./tmp\\media_complaint_day_2025-11-22.json\n",
      "2025-12-04 11:46:43,751 | INFO | Downloaded JSON from → ./tmp\\media_complaint_day_2025-11-23.json\n"
     ]
    }
   ],
   "source": [
    "# Social Media Complaints (Daily JSON)\n",
    "social_files = download_s3_prefix(SOCIAL_MEDIA_PREFIX, LOCAL_TMP, file_type=\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Website Complaints (Daily, from Postgres)\n",
    "#db_credentials = get_ssm_secret(POSTGRES_SSM_PARAMETER)\n",
    "\n",
    "#for table in POSTGRES_TABLES:\n",
    "#    df = extract_postgres_table(table, db_credentials)\n",
    "#    df.to_csv(os.path.join(LOCAL_TMP, f\"{table}.csv\"), index=False)\n",
    "\n",
    "#logging.info(\"All extractions completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "def get_ssm_parameter(name: str, decrypt: bool = True, region: str = \"eu-north-1\"):\n",
    "    \n",
    "    ssm = boto3.client(\"ssm\", region_name=AWS_REGION,\\\n",
    "                        aws_access_key_id=ACCESS_KEY_ID,\\\n",
    "                    aws_secret_access_key=SECRET_ACCESS_KEY)\n",
    "\n",
    "    try:\n",
    "        response = ssm.get_parameter(\n",
    "            Name=name,\n",
    "            WithDecryption=decrypt\n",
    "        )\n",
    "        return response[\"Parameter\"][\"Value\"]\n",
    "\n",
    "    except ClientError as e:\n",
    "        raise Exception(f\"SSM error retrieving '{name}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_ssm_secret(\"/coretelecomms/database/\")\n",
    "db_host = get_ssm_parameter(\"/coretelecomms/database/db_host\")\n",
    "db_name = get_ssm_parameter(\"/coretelecomms/database/db_name\")\n",
    "db_user = get_ssm_parameter(\"/coretelecomms/database/db_username\")\n",
    "db_pass = get_ssm_parameter(\"/coretelecomms/database/db_password\")\n",
    "db_port = get_ssm_parameter(\"/coretelecomms/database/db_port\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_12756\\1021591619.py:13: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "2025-12-04 21:42:07,903 | INFO | Extracted Postgres table → Web_form_request_2025_11_20\n",
      "2025-12-04 21:42:11,808 | INFO | Extracted Postgres table → Web_form_request_2025_11_21\n",
      "2025-12-04 21:42:14,394 | INFO | Extracted Postgres table → Web_form_request_2025_11_22\n",
      "2025-12-04 21:42:16,280 | INFO | Extracted Postgres table → Web_form_request_2025_11_23\n",
      "2025-12-04 21:42:16,284 | INFO | All extractions completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Fetch database credentials from SSM\n",
    "db_credentials = {\n",
    "    \"host\": get_ssm_parameter(\"/coretelecomms/database/db_host\"),\n",
    "    \"database\": get_ssm_parameter(\"/coretelecomms/database/db_name\"),\n",
    "    \"user\": get_ssm_parameter(\"/coretelecomms/database/db_username\"),\n",
    "    \"password\": get_ssm_parameter(\"/coretelecomms/database/db_password\"),\n",
    "    \"port\": get_ssm_parameter(\"/coretelecomms/database/db_port\"),\n",
    "}\n",
    "\n",
    "# Website Complaints (Daily, from Postgres)\n",
    "for table in POSTGRES_TABLES:\n",
    "    df = extract_postgres_table(table, db_credentials)\n",
    "    df.to_csv(os.path.join(LOCAL_TMP, f\"{table}.csv\"), index=False)\n",
    "\n",
    "logging.info(\"All extractions completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRANSFORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# import os\n",
    "# import pandas as pd\n",
    "\n",
    "TMP_FOLDER = LOCAL_TMP\n",
    "\n",
    "def transform_media_complaint_jsons(tmp_folder: str = TMP_FOLDER):\n",
    "    \"\"\"\n",
    "    Reads all JSON files that start with 'media_complaint' from the tmp folder,\n",
    "    transforms column names (lowercase, replace spaces with underscore),\n",
    "    converts date columns to datetime, and returns ONE combined DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    json_files = [\n",
    "        f for f in os.listdir(tmp_folder)\n",
    "        if f.startswith(\"media_complaint\") and f.endswith(\".json\")\n",
    "    ]\n",
    "\n",
    "    if not json_files:\n",
    "        print(\"No media_complaint JSON files found.\")\n",
    "        return pd.DataFrame()  # return empty dataframe\n",
    "\n",
    "    df_list = []\n",
    "\n",
    "    for file in json_files:\n",
    "        file_path = os.path.join(tmp_folder, file)\n",
    "\n",
    "        # Load JSON\n",
    "        df = pd.read_json(file_path)\n",
    "\n",
    "        # --- Transform Column Names ---\n",
    "        df.columns = (\n",
    "            df.columns\n",
    "            .str.lower()\n",
    "            .str.strip()\n",
    "            .str.replace(\" \", \"_\")\n",
    "        )\n",
    "\n",
    "        # --- Convert All Date Columns to datetime ---\n",
    "        for col in df.columns:\n",
    "            if \"date\" in col:\n",
    "                df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "\n",
    "        df_list.append(df)\n",
    "\n",
    "    # Combine all dataframes into one\n",
    "    final_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_complains = transform_media_complaint_jsons()\n",
    "media_complains.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_complains.to_parquet(\"tmp/media_complains.parquet\", compression=\"snappy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TMP_FOLDER = LOCAL_TMP\n",
    "\n",
    "def transform_web_form_csvs(tmp_folder: str = TMP_FOLDER):\n",
    "    \"\"\"\n",
    "    Reads all CSV files starting with 'web_form' from the tmp folder,\n",
    "    transforms column names (lowercase, underscores, no spaces),\n",
    "    converts date columns to datetime format,\n",
    "    and returns ONE combined DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    csv_files = [\n",
    "        f for f in os.listdir(tmp_folder)\n",
    "        if f.startswith(\"Web_form\") and f.endswith(\".csv\")\n",
    "    ]\n",
    "\n",
    "    if not csv_files:\n",
    "        print(\"No web_form CSV files found.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df_list = []\n",
    "\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(tmp_folder, file)\n",
    "\n",
    "        # Load CSV\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # --- Transform Column Names ---\n",
    "        df.columns = (\n",
    "            df.columns\n",
    "            .str.lower()\n",
    "            .str.strip()\n",
    "            .str.replace(\" \", \"_\")\n",
    "        )\n",
    "\n",
    "        # --- Convert Date Columns ---\n",
    "        for col in df.columns:\n",
    "            if \"date\" in col:\n",
    "                df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "\n",
    "        df_list.append(df)\n",
    "\n",
    "    # Combine all transformed CSV files\n",
    "    final_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column1</th>\n",
       "      <th>request_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>complaint_category</th>\n",
       "      <th>agent_id</th>\n",
       "      <th>resolutionstatus</th>\n",
       "      <th>request_date</th>\n",
       "      <th>resolution_date</th>\n",
       "      <th>webformgenerationdate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>nnc0ddcdX7446ddcd260d4520a53e7279611b32c637796...</td>\n",
       "      <td>7446ddcd-260d-4520-a53e-7279611b32c6</td>\n",
       "      <td>Payments</td>\n",
       "      <td>1071</td>\n",
       "      <td>In-Progress</td>\n",
       "      <td>2025-02-10 12:22:54</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2025-11-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>in39383bKd1b6383b601f4f39a97f9c2a158d02d41492a...</td>\n",
       "      <td>d1b6383b-601f-4f39-a97f-9c2a158d02d4</td>\n",
       "      <td>Router Delivery</td>\n",
       "      <td>1080</td>\n",
       "      <td>In-Progress</td>\n",
       "      <td>2025-03-07 18:46:16</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2025-11-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>ondd3cddQ012b3cdd17994d2db633c1f3845b3b7b95338...</td>\n",
       "      <td>012b3cdd-1799-4d2d-b633-c1f3845b3b7b</td>\n",
       "      <td>Router Delivery</td>\n",
       "      <td>1004</td>\n",
       "      <td>Resolved</td>\n",
       "      <td>2025-03-30 22:14:24</td>\n",
       "      <td>2025-04-12 12:52:24</td>\n",
       "      <td>2025-11-20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   column1                                         request_id  \\\n",
       "0        1  nnc0ddcdX7446ddcd260d4520a53e7279611b32c637796...   \n",
       "1        3  in39383bKd1b6383b601f4f39a97f9c2a158d02d41492a...   \n",
       "2        9  ondd3cddQ012b3cdd17994d2db633c1f3845b3b7b95338...   \n",
       "\n",
       "                            customer_id complaint_category  agent_id  \\\n",
       "0  7446ddcd-260d-4520-a53e-7279611b32c6           Payments      1071   \n",
       "1  d1b6383b-601f-4f39-a97f-9c2a158d02d4    Router Delivery      1080   \n",
       "2  012b3cdd-1799-4d2d-b633-c1f3845b3b7b    Router Delivery      1004   \n",
       "\n",
       "  resolutionstatus        request_date     resolution_date  \\\n",
       "0      In-Progress 2025-02-10 12:22:54                 NaT   \n",
       "1      In-Progress 2025-03-07 18:46:16                 NaT   \n",
       "2         Resolved 2025-03-30 22:14:24 2025-04-12 12:52:24   \n",
       "\n",
       "  webformgenerationdate  \n",
       "0            2025-11-20  \n",
       "1            2025-11-20  \n",
       "2            2025-11-20  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_form = transform_web_form_csvs()\n",
    "web_form.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_form.to_parquet(\"tmp/web_form.parquet\", compression=\"snappy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TMP_FOLDER = LOCAL_TMP\n",
    "\n",
    "def transform_call_logs_csv(tmp_folder: str = TMP_FOLDER):\n",
    "    \"\"\"\n",
    "    Reads all CSV files starting with 'call_logs' from tmp folder, \n",
    "    performs required transformations and returns ONE combined DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    csv_files = [\n",
    "        f for f in os.listdir(tmp_folder)\n",
    "        if f.startswith(\"call_logs\") and f.endswith(\".csv\")\n",
    "    ]\n",
    "\n",
    "    if not csv_files:\n",
    "        print(\"No call_logs CSV files found.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df_list = []\n",
    "\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(tmp_folder, file)\n",
    "\n",
    "        # Load CSV\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # --- Fix broken column name ---\n",
    "        # Handles cases like \"COMPLAINT_catego ry\" with space in between\n",
    "        bad_cols = [\"COMPLAINT_catego ry\", \"COMPLAINT_catego  ry\", \"COMPLAINT_category \"]\n",
    "        for bad_col in bad_cols:\n",
    "            if bad_col in df.columns:\n",
    "                df = df.rename(columns={bad_col: \"complaint_category\"})\n",
    "\n",
    "        # --- Generic column name cleaning ---\n",
    "        df.columns = (\n",
    "            df.columns\n",
    "            .str.strip()\n",
    "            .str.lower()\n",
    "            .str.replace(\" \", \"_\")\n",
    "        )\n",
    "\n",
    "        # --- Convert date-like columns to datetime ---\n",
    "        for col in df.columns:\n",
    "            if \"date\" in col:\n",
    "                df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "\n",
    "        df_list.append(df)\n",
    "\n",
    "    # Combine all transformed dfs into one\n",
    "    final_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unnamed:_0</th>\n",
       "      <th>call_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>complaint_category</th>\n",
       "      <th>agent_id</th>\n",
       "      <th>call_start_time</th>\n",
       "      <th>call_end_time</th>\n",
       "      <th>resolutionstatus</th>\n",
       "      <th>calllogsgenerationdate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Mar0979K65d7920296c349eab5435e5b1aeb39ce1476b1...</td>\n",
       "      <td>65d79202-96c3-49ea-b543-5e5b1aeb39ce</td>\n",
       "      <td>Technician Support</td>\n",
       "      <td>1035</td>\n",
       "      <td>2025-03-07 18:40:43</td>\n",
       "      <td>2025-03-07 18:52:43</td>\n",
       "      <td>Blocked</td>\n",
       "      <td>2025-11-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>CorcbaaQaf3aacc510074bd39f0ee5f0ca1bc9f47760ca...</td>\n",
       "      <td>af3aacc5-1007-4bd3-9f0e-e5f0ca1bc9f4</td>\n",
       "      <td>Payments</td>\n",
       "      <td>1021</td>\n",
       "      <td>2025-01-29 02:58:12</td>\n",
       "      <td>2025-01-29 03:03:12</td>\n",
       "      <td>In-Progress</td>\n",
       "      <td>2025-11-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Sar4328K5f528143241b4333ba13681987e4e0f7923987...</td>\n",
       "      <td>5f528143-241b-4333-ba13-681987e4e0f7</td>\n",
       "      <td>Payments</td>\n",
       "      <td>1031</td>\n",
       "      <td>2025-05-14 13:42:52</td>\n",
       "      <td>2025-05-14 13:55:52</td>\n",
       "      <td>Resolved</td>\n",
       "      <td>2025-11-20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   unnamed:_0                                            call_id  \\\n",
       "0           0  Mar0979K65d7920296c349eab5435e5b1aeb39ce1476b1...   \n",
       "1           2  CorcbaaQaf3aacc510074bd39f0ee5f0ca1bc9f47760ca...   \n",
       "2           3  Sar4328K5f528143241b4333ba13681987e4e0f7923987...   \n",
       "\n",
       "                            customer_id  complaint_category  agent_id  \\\n",
       "0  65d79202-96c3-49ea-b543-5e5b1aeb39ce  Technician Support      1035   \n",
       "1  af3aacc5-1007-4bd3-9f0e-e5f0ca1bc9f4            Payments      1021   \n",
       "2  5f528143-241b-4333-ba13-681987e4e0f7            Payments      1031   \n",
       "\n",
       "       call_start_time        call_end_time resolutionstatus  \\\n",
       "0  2025-03-07 18:40:43  2025-03-07 18:52:43          Blocked   \n",
       "1  2025-01-29 02:58:12  2025-01-29 03:03:12      In-Progress   \n",
       "2  2025-05-14 13:42:52  2025-05-14 13:55:52         Resolved   \n",
       "\n",
       "  calllogsgenerationdate  \n",
       "0             2025-11-20  \n",
       "1             2025-11-20  \n",
       "2             2025-11-20  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_logs = transform_call_logs_csv()\n",
    "call_logs.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "call_logs.to_parquet(\"tmp/call_logs.parquet\", compression=\"snappy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download_customer_csvs(\n",
    "    bucket: str = \"core-telecoms-data-lake\",\n",
    "    prefix: str = \"customers/\",\n",
    "    local_dir: str = LOCAL_TMP\n",
    "):\n",
    "    \"\"\"\n",
    "    Downloads all CSV files from a specific S3 prefix into a local directory\n",
    "    without listing the entire bucket (only objects under the prefix).\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure local folder exists\n",
    "    os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "    s3 = boto3.client(\"s3\", region_name=AWS_REGION,\\\n",
    "                        aws_access_key_id=ACCESS_KEY_ID,\\\n",
    "                    aws_secret_access_key=SECRET_ACCESS_KEY)\n",
    "\n",
    "    # List *only* objects under the prefix\n",
    "    response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "\n",
    "    if \"Contents\" not in response:\n",
    "        print(\"No files found under the prefix.\")\n",
    "        return\n",
    "\n",
    "    for obj in response[\"Contents\"]:\n",
    "        key = obj[\"Key\"]\n",
    "\n",
    "        # Only download CSV files\n",
    "        if key.lower().endswith(\".csv\"):\n",
    "            filename = os.path.basename(key)\n",
    "            local_path = os.path.join(local_dir, filename)\n",
    "\n",
    "            print(f\"Downloading {key} → {local_path}\")\n",
    "            s3.download_file(bucket, key, local_path)\n",
    "\n",
    "    print(\"All CSV files downloaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading customers/customers_dataset.csv → ./tmp\\customers_dataset.csv\n",
      "All CSV files downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "download_customer_csvs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_12756\\2665186842.py:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "  df = pd.read_csv(\"tmp\\customers_dataset.csv\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>name</th>\n",
       "      <th>Gender</th>\n",
       "      <th>DATE of biRTH</th>\n",
       "      <th>signup_date</th>\n",
       "      <th>email</th>\n",
       "      <th>address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d6966473-6fb0-4045-b33d-1f3b555e2762</td>\n",
       "      <td>Brianna Schultz</td>\n",
       "      <td>F</td>\n",
       "      <td>1930-09-10</td>\n",
       "      <td>2017-08-22</td>\n",
       "      <td>2772brianna_schultz525@GmaiL.om</td>\n",
       "      <td>317 Paul Turnpike\\nDarrenland, NY 13297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c3431b44-ebcd-4a36-ad0a-11a7cafbd0fd</td>\n",
       "      <td>April Morgan</td>\n",
       "      <td>F</td>\n",
       "      <td>1969-12-05</td>\n",
       "      <td>2017-12-08</td>\n",
       "      <td>2143april.morgane38@gmail.com</td>\n",
       "      <td>63925 Matthew Crescent Suite 075\\nNew Tylerbur...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            customer_id             name Gender DATE of biRTH  \\\n",
       "0  d6966473-6fb0-4045-b33d-1f3b555e2762  Brianna Schultz      F    1930-09-10   \n",
       "1  c3431b44-ebcd-4a36-ad0a-11a7cafbd0fd     April Morgan      F    1969-12-05   \n",
       "\n",
       "  signup_date                            email  \\\n",
       "0  2017-08-22  2772brianna_schultz525@GmaiL.om   \n",
       "1  2017-12-08    2143april.morgane38@gmail.com   \n",
       "\n",
       "                                             address  \n",
       "0            317 Paul Turnpike\\nDarrenland, NY 13297  \n",
       "1  63925 Matthew Crescent Suite 075\\nNew Tylerbur...  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"tmp\\customers_dataset.csv\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['customer_id', 'name', 'Gender', 'DATE of biRTH', 'signup_date',\n",
       "       'email', 'address'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "TMP_FOLDER = LOCAL_TMP\n",
    "\n",
    "def transform_customers_csv(tmp_folder: str = TMP_FOLDER):\n",
    "    \"\"\"\n",
    "    Reads all CSV files starting with 'call_logs' from tmp folder, \n",
    "    performs required transformations and returns ONE combined DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    csv_files = [\n",
    "        f for f in os.listdir(tmp_folder)\n",
    "        if f.startswith(\"customers\") and f.endswith(\".csv\")\n",
    "    ]\n",
    "\n",
    "    if not csv_files:\n",
    "        print(\"No call_logs CSV files found.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df_list = []\n",
    "\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(tmp_folder, file)\n",
    "\n",
    "        # Load CSV\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # --- Fix broken column name ---\n",
    "        # Handles cases like \"COMPLAINT_catego ry\" with space in between\n",
    "        bad_cols = [\"COMPLAINT_catego ry\", \"COMPLAINT_catego  ry\", \"COMPLAINT_category \"]\n",
    "        for bad_col in bad_cols:\n",
    "            if bad_col in df.columns:\n",
    "                df = df.rename(columns={bad_col: \"complaint_category\"})\n",
    "\n",
    "        # --- Generic column name cleaning ---\n",
    "        df.columns = (\n",
    "            df.columns\n",
    "            .str.strip()\n",
    "            .str.lower()\n",
    "            .str.replace(\" \", \"_\")\n",
    "        )\n",
    "\n",
    "        # --- Convert date-like columns to datetime ---\n",
    "        for col in df.columns:\n",
    "            if \"date\" in col:\n",
    "                df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "\n",
    "        df_list.append(df)\n",
    "\n",
    "    # Combine all transformed dfs into one\n",
    "    final_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers=transform_customers_csv()\n",
    "customers.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.to_parquet(\"tmp/customers.parquet\", compression=\"snappy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "def ensure_bucket_exists(bucket_name, region=AWS_REGION):\n",
    "    s3 = boto3.client(\"s3\", region_name=AWS_REGION,\\\n",
    "        aws_access_key_id=ACCESS_KEY_ID,\\\n",
    "        aws_secret_access_key=SECRET_ACCESS_KEY)\n",
    "\n",
    "    # Check if bucket exists\n",
    "    try:\n",
    "        s3.head_bucket(Bucket=bucket_name)\n",
    "        print(f\"Bucket '{bucket_name}' already exists.\")\n",
    "        return\n",
    "    except ClientError as e:\n",
    "        error_code = int(e.response['Error']['Code'])\n",
    "        if error_code != 404:\n",
    "            raise e\n",
    "\n",
    "    # If not exists → create it\n",
    "    print(f\"Creating bucket '{bucket_name}'...\")\n",
    "    if region == \"us-east-1\":\n",
    "        s3.create_bucket(Bucket=bucket_name)\n",
    "    else:\n",
    "        s3.create_bucket(\n",
    "            Bucket=bucket_name,\n",
    "            CreateBucketConfiguration={\"LocationConstraint\": region}\n",
    "        )\n",
    "    print(\"Bucket created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_parquet_files(local_dir, bucket_name, s3_prefix=\"\"):\n",
    "    s3 = boto3.client(\"s3\", region_name=AWS_REGION,\\\n",
    "        aws_access_key_id=ACCESS_KEY_ID,\\\n",
    "        aws_secret_access_key=SECRET_ACCESS_KEY)\n",
    "\n",
    "    # Ensure the bucket exists\n",
    "    ensure_bucket_exists(bucket_name)\n",
    "\n",
    "    # Loop through local parquet files\n",
    "    for file_name in os.listdir(local_dir):\n",
    "        if file_name.endswith(\".parquet\"):\n",
    "            local_path = os.path.join(local_dir, file_name)\n",
    "            s3_path = f\"{s3_prefix}/{file_name}\" if s3_prefix else file_name\n",
    "\n",
    "            try:\n",
    "                print(f\"Uploading {local_path} -> s3://{bucket_name}/{s3_path}\")\n",
    "                s3.upload_file(local_path, bucket_name, s3_path)\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR uploading {file_name}: {e}\")\n",
    "\n",
    "    print(\"Upload complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating bucket 'supabase-bucket-2025'...\n",
      "Bucket created.\n",
      "Uploading ./tmp\\call_logs.parquet -> s3://supabase-bucket-2025/raw/call_logs.parquet\n",
      "ERROR uploading call_logs.parquet: SSL validation failed for https://supabase-bucket-2025.s3.eu-north-1.amazonaws.com/raw/call_logs.parquet?uploadId=RDFwEOaV48ii1dbvcca5Qo8PKx4XpRHjCQhIZfsjDvnuXKJuojSlKzH5wYCgKl9asAcoFWqn6bto_NdAI3OO_q06OiWTdX.wV3n1bdsqjoSQai._ka29xO3XEbENKXBS&partNumber=5 EOF occurred in violation of protocol (_ssl.c:2417)\n",
      "Uploading ./tmp\\customers.parquet -> s3://supabase-bucket-2025/raw/customers.parquet\n",
      "ERROR uploading customers.parquet: Could not connect to the endpoint URL: \"https://supabase-bucket-2025.s3.eu-north-1.amazonaws.com/raw/customers.parquet?uploadId=3jUJr7igcbxAnFfez4thelM33EjX6pX6MBu2fUZfk8uqGYeU65DpeKGYrscO0jd54nHo.sc3ubXis3f5Qo6B2WjPBx5IiM1_.9jE4Ro93tWXBVGN6_umchCRyAA5ngS6&partNumber=11\"\n",
      "Uploading ./tmp\\media_complains.parquet -> s3://supabase-bucket-2025/raw/media_complains.parquet\n",
      "Uploading ./tmp\\web_form.parquet -> s3://supabase-bucket-2025/raw/web_form.parquet\n",
      "Upload complete!\n"
     ]
    }
   ],
   "source": [
    "upload_parquet_files(\n",
    "        local_dir=LOCAL_TMP,\n",
    "        bucket_name=\"supabase-bucket-2025\",\n",
    "        s3_prefix=\"raw\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b86aa247c6762e1192ef30d4a148d9cb2e1093f7e81dd7f812977548b8bc96e6"
  },
  "kernelspec": {
   "display_name": "Python 3.12.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
